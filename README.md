This repository contains the code and data for our research project on the Decision Transformer (DT) as a booster for other learners in a reinforcement learning setting.

# Introduction
Reinforcement Learning (RL) is a powerful paradigm for developing agents that can learn to make decisions in complex environments. However, RL algorithms can be slow and sample inefficient when learning through online interaction with the environment. The Decision Transformer (DT) offers a promising alternative by learning offline from a dataset of trajectories. DT has shown impressive results in several domains, including Atari games, but it requires access to an expert dataset of trajectories.

In this research project, we investigate whether the DT can be used as a booster for other learners in a reinforcement learning setting. Specifically, we aim to collect and use two datasets of trajectories, one containing random trajectories and the other containing expert trajectories generated by a Q-learning agent. We will then train the DT on these datasets and compare its performance with that of a Q-learning agent and a more sophisticated expert learner, the Deep Q-Network (DQN).

# Research Questions
Can the DT effectively learn from the expert dataset of trajectories to outperform other learners?
How do the hyperparameters of the DT affect its performance, and can we identify optimal hyperparameters?
Can the DT be used as a booster for other learners in a reinforcement learning setting, and does it provide a performance gain over the baseline learners?
